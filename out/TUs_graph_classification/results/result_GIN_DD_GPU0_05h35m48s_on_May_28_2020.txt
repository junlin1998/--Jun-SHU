Dataset: DD,
Model: GIN

params={'seed': 41, 'epochs': 1000, 'batch_size': 50, 'init_lr': 0.0005, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 25, 'min_lr': 1e-06, 'weight_decay': 0, 'print_epoch_interval': 5, 'max_time': 48}

net_params={'device': device(type='cuda', index=0), 'gated': False, 'in_dim': 89, 'residual': True, 'hidden_dim': 110, 'out_dim': 110, 'n_classes': 2, 'n_heads': -1, 'L': 4, 'readout': 'mean', 'graph_norm': True, 'batch_norm': True, 'in_feat_dropout': 0.0, 'dropout': 0.0, 'edge_feat': False, 'self_loop': False, 'pseudo_dim_MoNet': -1, 'kernel': -1, 'n_mlp_GIN': 2, 'learn_eps_GIN': True, 'neighbor_aggr_GIN': 'sum', 'sage_aggregator': 'meanpool', 'data_mode': 'default', 'gnn_per_block': -1, 'embedding_dim': -1, 'pool_ratio': -1, 'linkpred': True, 'num_pool': 1, 'cat': False, 'batch_size': 50, 'assign_dim': -287400, 'gpu_id': 0, 'total_param': 111334}

GINNet(
  (ginlayers): ModuleList(
    (0): GINLayer(
      (apply_func): ApplyNodeFunc(
        (mlp): MLP(
          (linears): ModuleList(
            (0): Linear(in_features=110, out_features=110, bias=True)
            (1): Linear(in_features=110, out_features=110, bias=True)
          )
          (batch_norms): ModuleList(
            (0): BatchNorm1d(110, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (bn): BatchNorm1d(110, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (bn_node_h): BatchNorm1d(110, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): GINLayer(
      (apply_func): ApplyNodeFunc(
        (mlp): MLP(
          (linears): ModuleList(
            (0): Linear(in_features=110, out_features=110, bias=True)
            (1): Linear(in_features=110, out_features=110, bias=True)
          )
          (batch_norms): ModuleList(
            (0): BatchNorm1d(110, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (bn): BatchNorm1d(110, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (bn_node_h): BatchNorm1d(110, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): GINLayer(
      (apply_func): ApplyNodeFunc(
        (mlp): MLP(
          (linears): ModuleList(
            (0): Linear(in_features=110, out_features=110, bias=True)
            (1): Linear(in_features=110, out_features=110, bias=True)
          )
          (batch_norms): ModuleList(
            (0): BatchNorm1d(110, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (bn): BatchNorm1d(110, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (bn_node_h): BatchNorm1d(110, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): GINLayer(
      (apply_func): ApplyNodeFunc(
        (mlp): MLP(
          (linears): ModuleList(
            (0): Linear(in_features=110, out_features=110, bias=True)
            (1): Linear(in_features=110, out_features=110, bias=True)
          )
          (batch_norms): ModuleList(
            (0): BatchNorm1d(110, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (bn): BatchNorm1d(110, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (bn_node_h): BatchNorm1d(110, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (embedding_h): Linear(in_features=89, out_features=110, bias=True)
  (linears_prediction): ModuleList(
    (0): Linear(in_features=110, out_features=2, bias=True)
    (1): Linear(in_features=110, out_features=2, bias=True)
    (2): Linear(in_features=110, out_features=2, bias=True)
    (3): Linear(in_features=110, out_features=2, bias=True)
    (4): Linear(in_features=110, out_features=2, bias=True)
  )
  (pool): AvgPooling()
)

Total Parameters: 111334


    FINAL RESULTS
TEST ACCURACY averaged: 76.1506 with s.d. 2.4598
TRAIN ACCURACY averaged: 100.0000 with s.d. 0.0000


    Total Time Taken: 1.3841 hrs
Average Time Per Epoch: 2.0397 s


All Splits Test Accuracies: [0.7815126050420168, 0.7627118644067796, 0.7372881355932204, 0.7288135593220338, 0.7288135593220338, 0.7542372881355932, 0.7627118644067796, 0.811965811965812, 0.7692307692307693, 0.7777777777777778]